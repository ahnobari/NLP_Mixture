{"math": {"num_samples": 5000, "num_scores": 5000, "timeout_samples": 0, "empty_samples": 179, "acc": 51.9, "type_acc": {"Algebra": 70.3, "Counting & Probability": 50.8, "Geometry": 42.8, "Intermediate Algebra": 30.1, "Number Theory": 47.0, "Prealgebra": 66.8, "Precalculus": 37.7}}, "gsm8k": {"num_samples": 1319, "num_scores": 1319, "timeout_samples": 0, "empty_samples": 73, "acc": 81.0}, "math_oai": {"num_samples": 500, "num_scores": 500, "timeout_samples": 0, "empty_samples": 23, "acc": 47.8}, "gsm_hard": {"num_samples": 1319, "num_scores": 1319, "timeout_samples": 0, "empty_samples": 77, "acc": 39.9}, "mbpp": {"base": {"pass@1": 0.291005291005291}, "+": {"pass@1": 0.23809523809523808}}, "mmlu": {"abstract_algebra": 0.37, "anatomy": 0.43703703703703706, "astronomy": 0.5131578947368421, "business_ethics": 0.47, "clinical_knowledge": 0.6264150943396226, "college_biology": 0.5625, "college_chemistry": 0.39, "college_computer_science": 0.47, "college_mathematics": 0.41, "college_medicine": 0.5606936416184971, "college_physics": 0.29411764705882354, "computer_security": 0.65, "conceptual_physics": 0.44680851063829785, "econometrics": 0.3508771929824561, "electrical_engineering": 0.5103448275862069, "elementary_mathematics": 0.43386243386243384, "formal_logic": 0.42063492063492064, "global_facts": 0.35, "high_school_biology": 0.6645161290322581, "high_school_chemistry": 0.4088669950738916, "high_school_computer_science": 0.55, "high_school_european_history": 0.6303030303030303, "high_school_geography": 0.7171717171717171, "high_school_government_and_politics": 0.7098445595854922, "high_school_macroeconomics": 0.5461538461538461, "high_school_mathematics": 0.3851851851851852, "high_school_microeconomics": 0.5714285714285714, "high_school_physics": 0.3973509933774834, "high_school_psychology": 0.7302752293577982, "high_school_statistics": 0.5185185185185185, "high_school_us_history": 0.6274509803921569, "high_school_world_history": 0.6962025316455697, "human_aging": 0.4798206278026906, "human_sexuality": 0.6564885496183206, "international_law": 0.6528925619834711, "jurisprudence": 0.7129629629629629, "logical_fallacies": 0.5460122699386503, "machine_learning": 0.4107142857142857, "management": 0.6893203883495146, "marketing": 0.7478632478632479, "medical_genetics": 0.61, "miscellaneous": 0.6883780332056194, "moral_disputes": 0.5953757225433526, "moral_scenarios": 0.29832402234636873, "nutrition": 0.5686274509803921, "philosophy": 0.5691318327974276, "prehistory": 0.5308641975308642, "professional_accounting": 0.35106382978723405, "professional_law": 0.34810951760104303, "professional_medicine": 0.5551470588235294, "professional_psychology": 0.48856209150326796, "public_relations": 0.6090909090909091, "security_studies": 0.6, "sociology": 0.6965174129353234, "us_foreign_policy": 0.74, "virology": 0.4457831325301205, "world_religions": 0.7017543859649122, "overall": 0.5221478421877226}}