{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mbpp = load_dataset(\"mbpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 11,\n",
       " 'text': 'Write a python function to remove first and last occurrence of a given character from the string.',\n",
       " 'code': 'def remove_Occ(s,ch): \\r\\n    for i in range(len(s)): \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    for i in range(len(s) - 1,-1,-1):  \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    return s ',\n",
       " 'test_list': ['assert remove_Occ(\"hello\",\"l\") == \"heo\"',\n",
       "  'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"',\n",
       "  'assert remove_Occ(\"PHP\",\"P\") == \"H\"'],\n",
       " 'test_setup_code': '',\n",
       " 'challenge_test_list': ['assert remove_Occ(\"hellolloll\",\"l\") == \"helollol\"',\n",
       "  'assert remove_Occ(\"\",\"l\") == \"\"']}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbpp['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "def load_model_and_tokenizer(model_name = \"google/gemma-2b-it\", device = None):\n",
    "    \n",
    "    if device is None:\n",
    "      device = 'cpu'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                              trust_remote_code=True,\n",
    "                                              )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device, torch_dtype=torch.bfloat16)\n",
    "    model.eval()\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer('meta-llama/Llama-3.2-1B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def predict_code(model, tokenizer, problem_description):\n",
    "\n",
    "    prompt = [{\"role\":\"user\", \"content\": f\"Problem: {problem_description}\\n\\n\"\n",
    "            \"Input:\\nWrite a single Python function to solve the problem above.\\nOutput:\\n\"}] \n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    print(outputs)\n",
    "    predicted_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return predicted_code[len(prompt):]\n",
    "\n",
    "def parse_test_case(test_case):\n",
    "    \"\"\"\n",
    "    Parse an assertion like:\n",
    "    assert max_chain_length([Pair(1, 2), Pair(3, 4)], 4) == 4\n",
    "    to extract:\n",
    "    - Function call as a string: \"max_chain_length([Pair(1, 2), Pair(3, 4)], 4)\"\n",
    "    - Expected output: 4\n",
    "    \"\"\"\n",
    "    # Parse the assertion string\n",
    "    assertion_node = ast.parse(test_case, mode='exec').body[0]\n",
    "    if isinstance(assertion_node, ast.Assert):\n",
    "        test_call = assertion_node.test.left  # The function call (left of ==)\n",
    "        # i want to take out the name of the function too\n",
    "        test_call = test_call.args\n",
    "        expected_output = assertion_node.test.comparators[0]  # The value (right of ==)\n",
    "\n",
    "        # Convert AST back to source code for evaluation\n",
    "        test_call_code = ast.unparse(test_call)\n",
    "        expected_output_value = eval(ast.unparse(expected_output))\n",
    "        return test_call_code, expected_output_value\n",
    "    else:\n",
    "        raise ValueError(\"Test case is not a valid assertion.\")\n",
    "    \n",
    "\n",
    "def evaluate_problem(function, test_cases):\n",
    "\n",
    "    success_count = 0\n",
    "    local_env = {}\n",
    "    exec(function, {}, local_env)\n",
    "    function_handle = next(iter(local_env.values()))\n",
    "    for test_case in test_cases:\n",
    "        input_data, expected_output = parse_test_case(test_case)\n",
    "        try:\n",
    "            result = function_handle(eval(input_data))\n",
    "            if result == expected_output:\n",
    "                success_count += 1\n",
    "        except Exception as e:\n",
    "            print(\"not a python code\")\n",
    "            continue\n",
    "\n",
    "    # Return results\n",
    "    accuracy = success_count / len(test_cases)\n",
    "    print(accuracy)\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m ground_truth_code \u001b[38;5;241m=\u001b[39m sample_problem[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m test_cases \u001b[38;5;241m=\u001b[39m sample_problem[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m predicted_code \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_code)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ground_truth_code)\n",
      "Cell \u001b[0;32mIn[93], line 10\u001b[0m, in \u001b[0;36mpredict_code\u001b[0;34m(model, tokenizer, problem_description)\u001b[0m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblem_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWrite a single Python function to solve the problem above.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}] \n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     12\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     13\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     15\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     16\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m predicted_code \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_code[\u001b[38;5;28mlen\u001b[39m(prompt):]\n",
      "\u001b[0;31mTypeError\u001b[0m: transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "source": [
    "def clean_code(predicted_code):\n",
    "    \"\"\"\n",
    "    Finds the first occurrence of 'def' in the predicted code and removes everything before it.\n",
    "    If 'def' is not found, returns the code unchanged.\n",
    "    \"\"\"\n",
    "    def_index = predicted_code.find(\"def\")  # Find the index of the first 'def'\n",
    "    if def_index != -1:\n",
    "        return predicted_code[def_index:]  # Return the substring starting from 'def'\n",
    "    return predicted_code  \n",
    "for i in range(0, 10):\n",
    "    print(\"Problem\", i)\n",
    "    sample_problem = mbpp['train'][i] \n",
    "    problem_description = sample_problem[\"text\"]\n",
    "    ground_truth_code = sample_problem[\"code\"]\n",
    "    test_cases = sample_problem[\"test_list\"]\n",
    "\n",
    "    predicted_code = predict_code(model, tokenizer, problem_description)\n",
    "    print(\"pred:\", predicted_code)\n",
    "    print(\"ground:\", ground_truth_code)\n",
    "    cleaned_code = clean_code(predicted_code)\n",
    "    print(\"cleaned:\", cleaned_code)\n",
    "    accuracy = evaluate_problem(cleaned_code, test_cases)\n",
    "    print(accuracy)\n",
    "# sample_problem = mbpp['train'][12] \n",
    "# problem_description = sample_problem[\"text\"]\n",
    "# ground_truth_code = sample_problem[\"code\"]\n",
    "# test_cases = sample_problem[\"test_list\"]\n",
    "\n",
    "\n",
    "# predicted_code = predict_code(model, tokenizer, problem_description)\n",
    "# accuracy = evaluate_problem(predicted_code, test_cases)\n",
    "# print(f\"Problem Description:\\n{problem_description}\")\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "# print(f\"Predicted Code:\\n{predicted_code}\")\n",
    "# print(f\"Ground Truth Code:\\n{ground_truth_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 12\n",
      "cleaned:\n",
      "\n",
      " def max_record_value(tuples_list):\n",
      "    \"\"\"\n",
      "    This function finds the maximum value in a list of tuples.\n",
      "\n",
      "    Args:\n",
      "    tuples_list (list): A list of tuples.\n",
      "\n",
      "    Returns:\n",
      "    tuple: The maximum value in the list of tuples.\n",
      "    \"\"\"\n",
      "    if not tuples_list:\n",
      "        return None  # Return None if the list is empty\n",
      "\n",
      "    max_value = max(tuples_list[0])  # Initialize max_value with the first tuple\n",
      "    for tuple in tuples_list[1:]:  # Iterate over the rest of the tuples\n",
      "        if tuple[0] > max_value:  # Compare the first element of the current tuple with max_value\n",
      "            max_value = tuple[0]  # Update max_value if the current tuple's first element is larger\n",
      "\n",
      "    return max_value  # Return the maximum value\n",
      "\n",
      "\n",
      "# Example usage\n",
      "tuples_list = [(10, 20, 30), (40, 50, 60), (70, 80, 90)]\n",
      "max_value = max_record_value(tuples_list)\n",
      "print(\"Maximum value:\", max_value)  # Output: Maximum value: (90, 60, 70)\n",
      "1.0\n",
      "v\n",
      "Maximum value: 70\n",
      "not a python code\n",
      "not a python code\n",
      "not a python code\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "i = 12\n",
    "print(\"Problem\", i)\n",
    "sample_problem = mbpp['train'][i] \n",
    "problem_description = sample_problem[\"text\"]\n",
    "ground_truth_code = sample_problem[\"code\"]\n",
    "test_cases = sample_problem[\"test_list\"]\n",
    "\n",
    "# predicted_code = predict_code(model, tokenizer, problem_description)\n",
    "# print(\"pred:\", predicted_code)\n",
    "# print(\"ground:\", ground_truth_code)\n",
    "\n",
    "# extract code inside ```python\n",
    "import re\n",
    "def extract_function(predicted_code):\n",
    "    pattern = r\"```python\\n(.*)\\n```\"\n",
    "    match = re.findall(pattern, predicted_code, re.DOTALL)\n",
    "    return match[-1]\n",
    "\n",
    "print(\"cleaned:\\n\\n\", extract_function(predicted_code))\n",
    "\n",
    "#execute the code\n",
    "#result = evaluate_problem(ground_truth_code, test_cases)\n",
    "#print(\"v\")\n",
    "#result = evaluate_problem(extract_function(predicted_code), test_cases)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# cleaned_code = extract_function(predicted_code)\n",
    "# print(\"cleaned:\", cleaned_code)\n",
    "# accuracy = evaluate_problem(cleaned_code, test_cases)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def max_record_value(tuples_list):\n",
      "    \"\"\"\n",
      "    This function finds the maximum value in a list of tuples.\n",
      "\n",
      "    Args:\n",
      "    tuples_list (list): A list of tuples.\n",
      "\n",
      "    Returns:\n",
      "    tuple: The maximum value in the list of tuples.\n",
      "    \"\"\"\n",
      "    if not tuples_list:\n",
      "        return None  # Return None if the list is empty\n",
      "\n",
      "    max_value = max(tuples_list[0])  # Initialize max_value with the first tuple\n",
      "    for tuple in tuples_list[1:]:  # Iterate over the rest of the tuples\n",
      "        if tuple[0] > max_value:  # Compare the first element of the current tuple with max_value\n",
      "            max_value = tuple[0]  # Update max_value if the current tuple's first element is larger\n",
      "\n",
      "    return max_value  # Return the maximum value\n",
      "\n",
      "\n",
      "# Example usage\n",
      "tuples_list = [(10, 20, 30), (40, 50, 60), (70, 80, 90)]\n",
      "max_value = max_record_value(tuples_list)\n",
      "print(\"Maximum value:\", max_value)  # Output: Maximum value: (90, 60, 70)\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "from EvalUtils.MathUtils.python_executor import PythonExecutor\n",
    "from tqdm import trange\n",
    "import re\n",
    "from timeout_decorator import timeout\n",
    "\n",
    "executor = PythonExecutor()\n",
    "\n",
    "@timeout(5, timeout_exception=ValueError)\n",
    "def run_code(code):\n",
    "    exec(code)\n",
    "\n",
    "def extract_python(predicted_code):\n",
    "    pattern = r\"```python\\n(.*)\\n```\"\n",
    "    match = re.findall(pattern, predicted_code, re.DOTALL)\n",
    "    if len(match) == 0:\n",
    "        return \"\"\n",
    "    return match[-1]\n",
    "\n",
    "def run_mbpp_benchmark(model, prompt_type, max_tokens=1024):\n",
    "    import datasets\n",
    "    data = datasets.load_dataset(\"mbpp\")['test']\n",
    "    \n",
    "    inputs = []\n",
    "    test_cases = []\n",
    "    for i in range(len(data)):\n",
    "        inputs.append([{ \"role\": \"user\", \"content\": data[i]['text']}])\n",
    "        test_cases.append(\"\\n\".join(data[i]['test_list']))\n",
    "\n",
    "    if prompt_type == 'chat':\n",
    "        inputs = model.get_tokenizer().apply_chat_template(inputs, add_generation_prompt=True, tokenize=False)\n",
    "    elif prompt_type == 'plain':\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError('problem_prompt should be either \"chat\" or \"plain\"')\n",
    "    \n",
    "\n",
    "    sampling = SamplingParams(n=1, temperature=0.0, max_tokens=max_tokens, top_p=1.0)\n",
    "    \n",
    "    results = model.generate(inputs, sampling_params=sampling)\n",
    "    results = extract_outputs(results)\n",
    "\n",
    "    accuracy = 0\n",
    "    total_codes = []\n",
    "    for i in trange(len(results)):\n",
    "        predicted_code = results[i]        \n",
    "        cleaned_code = extract_python(predicted_code)\n",
    "        \n",
    "        try:\n",
    "            function_name = cleaned_code.split('def ')[1].split('(')[0]\n",
    "            # add a timeout to the execution\n",
    "            test_function_name = test_cases[i].split('assert ')[1].split('(')[0]\n",
    "            test = test_cases[i].replace(test_function_name, function_name)\n",
    "            total_codes.append(cleaned_code+'\\n'+test)\n",
    "            # _,r = executor.execute(cleaned_code+'\\n'+test)\n",
    "            run_code(cleaned_code+'\\n'+test)\n",
    "            \n",
    "            accuracy += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return accuracy / len(results), results, test_cases, total_codes\n",
    "\n",
    "def extract_outputs(outputs):\n",
    "    return [output.outputs[0].text for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "data = datasets.load_dataset(\"mbpp\")['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:00:05 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-06 23:00:05 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-06 23:00:05 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=39000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 12-06 23:00:06 model_runner.py:1056] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 12-06 23:00:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee8268c02ae44519b288ffcd8eff6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 23:00:16 model_runner.py:1067] Loading model weights took 14.9888 GB\n",
      "INFO 12-06 23:00:16 gpu_executor.py:122] # GPU blocks: 2488, # CPU blocks: 2048\n",
      "INFO 12-06 23:00:16 gpu_executor.py:126] Maximum concurrency for 39000 tokens per request: 1.02x\n",
      "INFO 12-06 23:00:18 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-06 23:00:18 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-06 23:00:27 model_runner.py:1523] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "from EvalUtils import load_model\n",
    "\n",
    "model = load_model('meta-llama/Llama-3.1-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/500 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:00:44 scheduler.py:1483] Sequence group 255 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n",
      "WARNING 12-06 23:00:45 scheduler.py:1483] Sequence group 205 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n",
      "WARNING 12-06 23:00:47 scheduler.py:1483] Sequence group 155 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  21%|██        | 104/500 [00:19<00:41,  9.57it/s, est. speed input: 272.60 toks/s, output: 1975.85 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:00:58 scheduler.py:1483] Sequence group 213 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  31%|███       | 154/500 [00:25<00:45,  7.57it/s, est. speed input: 309.74 toks/s, output: 2442.47 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:01:05 scheduler.py:1483] Sequence group 291 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  37%|███▋      | 184/500 [00:29<00:50,  6.27it/s, est. speed input: 324.02 toks/s, output: 2596.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:01:08 scheduler.py:1483] Sequence group 321 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 210/500 [00:32<00:38,  7.46it/s, est. speed input: 332.58 toks/s, output: 2678.30 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:01:12 scheduler.py:1483] Sequence group 346 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  54%|█████▎    | 268/500 [00:39<00:19, 12.19it/s, est. speed input: 354.11 toks/s, output: 2890.92 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:01:18 scheduler.py:1483] Sequence group 446 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  55%|█████▌    | 277/500 [00:41<00:44,  5.02it/s, est. speed input: 347.02 toks/s, output: 2829.92 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:01:20 scheduler.py:1483] Sequence group 399 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  63%|██████▎   | 313/500 [00:45<00:22,  8.26it/s, est. speed input: 355.65 toks/s, output: 2935.24 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:01:24 scheduler.py:1483] Sequence group 465 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  67%|██████▋   | 337/500 [00:49<00:21,  7.53it/s, est. speed input: 355.89 toks/s, output: 2945.98 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 23:01:28 scheduler.py:1483] Sequence group 464 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 500/500 [01:10<00:00,  7.09it/s, est. speed input: 369.16 toks/s, output: 3122.09 toks/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hell, Wold!\n",
      "Pytho is fun\n",
      "Misissppi\n",
      "\n",
      "abc\n",
      "The volume of the triangular prism is: 37.5 cubic units\n",
      "True\n",
      "False\n",
      "[2, 4, 6, 8, 10]\n",
      "29\n",
      "[1, 4, 7]\n",
      "[2, 5, 8]\n",
      "['dog', 'cat']\n",
      "Original array: [4, 2, 2, 8, 3, 3, 1]\n",
      "Sorted array: [1, 2, 2, 3, 3, 4, 8]\n",
      "3\n",
      "Original array: [64, 34, 25, 12, 22, 11, 90]\n",
      "Sorted array: [11, 12, 22, 25, 34, 64, 90]\n",
      "True\n",
      "False\n",
      "There are 10 squares of size 2 that can fit in a rectangle of length 10 and width 5.\n",
      "-3\n",
      "20\n",
      "['3', '6', '9']\n",
      "Triplet found: [1, 5, 9]\n",
      " 8  1  6\n",
      " 3  5  7\n",
      " 4  9  2\n",
      "-12\n",
      "Even\n",
      "Total bill: $500.00\n",
      "Total bill with taxes: $550.00\n",
      "Original list: [23, 10, 20, 11, 12, 6, 7]\n",
      "Sorted list: [6, 7, 10, 11, 12, 20, 23]\n",
      "3\n",
      "ASCII sum: 1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 283.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2863311544\n",
      "2863311533\n",
      "Invalid date\n",
      "Summer\n",
      "Summer\n",
      "Invalid date\n",
      "{1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 0, 7: 0, 8: 1, 9: 0, 10: 0, 11: 0, 12: 3, 13: 0, 14: 0, 15: 2, 16: 0, 17: 0, 18: 1, 19: 0, 20: 0, 21: 0, 22: 0, 23: 1, 24: 0, 25: 0, 26: 0}\n",
      "0\n",
      "1\n",
      "0\n",
      "fl\n",
      "\n",
      "inter\n",
      "The focus of the parabola is: (0.0, 1.25)\n",
      "The decimal equivalent of 12 is 10\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "Original heap: [4, 2, 9, 6, 5, 1, 8, 3, 7]\n",
      "Smallest element: 4\n",
      "Heap after deleting smallest element: [2, 5, 9, 6, 7, 1, 8, 3]\n",
      "Inserting new item: 0\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "Maximum k elements: [8, 9, 10]\n",
      "Minimum k elements: [1, 2, 3]\n",
      "Hello_World\n",
      "2863311530\n",
      "2863311550\n",
      "18\n",
      "5\n",
      "10.5\n",
      "21.0\n",
      "33\n",
      "0\n",
      "(5, 15, 25)\n",
      "None\n",
      "Element 23 found at index 5.\n",
      "[5, 15, 25]\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "The number of odd days in 2024 is: 366\n",
      "[3, 5, 6, 8, 9]\n",
      "105\n",
      "[(1, 2), (2, 3), (3, 4), (4, 5)]\n",
      "True\n",
      "False\n",
      "40.0\n",
      "There are 10 squares of size 2 that fit in the rectangle.\n",
      "[1, 0, 0, 4]\n",
      "True\n",
      "True\n",
      "Lateral surface area: 62\n",
      "Original list: [64, 34, 25, 12, 22, 11, 90]\n",
      "[6, 7, 8, 9, 10]\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "(0, 5, 2)\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "10\n",
      "10\n",
      "Result: 110\n",
      "The average of cubes of the first 5 natural numbers is: 45.0\n",
      "[2, 4, 6, 8, 10]\n",
      "26-07-2022\n",
      "Original array: [64, 34, 25, 12, 22, 11, 90]\n",
      "Sorted array: [11, 12, 22, 25, 34, 64, 90]\n",
      "Elementwise AND result: (1, 2, 3, 0)\n",
      "Tuple-wise AND result: (1, 2, 3, 0)\n",
      "The directrix is a line with slope 0 and y-intercept -0.25\n",
      "[1, 8, 27, 64, 125]\n",
      "HelloWorld!Thisisateststring.\n",
      "112\n",
      "['Hello', 'Worldhello', 'World', 'Hello', 'World']\n",
      "Element 23 found at index 5.\n",
      "Hexagon at (0, 0): [(0, 0), (-0.5, 0.5), (0.5, 0.5), (1, 0), (0.5, -0.5), (-0.5, -0.5)]\n",
      "Hexagon at (1, 0): [(1, 0), (0.5, 0.5), (1.5, 0.5), (2, 0), (1.5, -0.5), (0.5, -0.5)]\n",
      "Hexagon at (2, 0): [(2, 0), (1.5, 0.5), (2.5, 0.5), (3, 0), (2.5, -0.5), (1.5, -0.5)]\n",
      "Hexagon at (0, 1): [(0, 1), (-0.5, 1.5), (0.5, 1.5), (1, 1), (0.5, 0.5), (-0.5, 0.5)]\n",
      "Hexagon at (1, 1): [(1, 1), (0.5, 1.5), (1.5, 1.5), (2, 1), (1.5, 0.5), (0.5, 0.5)]\n",
      "Hexagon at (2, 1): [(2, 1), (1.5, 1.5), (2.5, 1.5), (3, 1), (2.5, 0.5), (1.5, 0.5)]\n",
      "Hexagon at (0, 2): [(0, 2), (-0.5, 2.5), (0.5, 2.5), (1, 2), (0.5, 1.5), (-0.5, 1.5)]\n",
      "Hexagon at (1, 2): [(1, 2), (0.5, 2.5), (1.5, 2.5), (2, 2), (1.5, 1.5), (0.5, 1.5)]\n",
      "Hexagon at (2, 2): [(2, 2), (1.5, 2.5), (2.5, 2.5), (3, 2), (2.5, 1.5), (1.5, 1.5)]\n",
      "H, W! T    .\n",
      "The diameter of the circle with radius 5 is 10\n",
      "True\n",
      "True\n",
      "4.0\n",
      "5.0\n",
      "6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc,results, test_cases, total_codes = run_mbpp_benchmark(model, 'chat', max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assert remove_Occ(\"hello\",\"l\") == \"heo\"\n",
      "assert remove_Occ(\"abcda\",\"a\") == \"bcd\"\n",
      "assert remove_Occ(\"PHP\",\"P\") == \"H\" \n",
      " def remove_first_and_last_occurrence(input_string, char):\n",
      "    \"\"\"\n",
      "    Removes the first and last occurrence of a given character from the string.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The input string.\n",
      "        char (str): The character to be removed.\n",
      "\n",
      "    Returns:\n",
      "        str: The modified string with the first and last occurrence of the character removed.\n",
      "    \"\"\"\n",
      "    # Find the index of the first occurrence of the character\n",
      "    first_occurrence_index = input_string.find(char)\n",
      "    \n",
      "    # If the character is not found, return the original string\n",
      "    if first_occurrence_index == -1:\n",
      "        return input_string\n",
      "    \n",
      "    # Find the index of the last occurrence of the character\n",
      "    last_occurrence_index = input_string.rfind(char)\n",
      "    \n",
      "    # If the character is not found in the string, return the original string\n",
      "    if last_occurrence_index == -1:\n",
      "        return input_string\n",
      "    \n",
      "    # Remove the first and last occurrence of the character\n",
      "    modified_string = input_string[:first_occurrence_index] + input_string[last_occurrence_index + 1:]\n",
      "    \n",
      "    return modified_string\n",
      "\n",
      "# Example usage:\n",
      "input_str = \"Hello, World!\"\n",
      "char_to_remove = \"l\"\n",
      "print(remove_first_and_last_occurrence(input_str, char_to_remove))  # Output: \"Heo, World!\"\n",
      "assert remove_first_and_last_occurrence(\"hello\",\"l\") == \"heo\"\n",
      "assert remove_first_and_last_occurrence(\"abcda\",\"a\") == \"bcd\"\n",
      "assert remove_first_and_last_occurrence(\"PHP\",\"P\") == \"H\"\n"
     ]
    }
   ],
   "source": [
    "print(test_cases[0], '\\n', total_codes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python function that removes the first and last occurrence of a given character from a string:\n",
      "\n",
      "```python\n",
      "def remove_first_and_last_occurrence(input_string, char):\n",
      "    \"\"\"\n",
      "    Removes the first and last occurrence of a given character from the string.\n",
      "\n",
      "    Args:\n",
      "        input_string (str): The input string.\n",
      "        char (str): The character to be removed.\n",
      "\n",
      "    Returns:\n",
      "        str: The modified string with the first and last occurrence of the character removed.\n",
      "    \"\"\"\n",
      "    # Find the index of the first occurrence of the character\n",
      "    first_occurrence_index = input_string.find(char)\n",
      "    \n",
      "    # If the character is not found, return the original string\n",
      "    if first_occurrence_index == -1:\n",
      "        return input_string\n",
      "    \n",
      "    # Find the index of the last occurrence of the character\n",
      "    last_occurrence_index = input_string.rfind(char)\n",
      "    \n",
      "    # If the character is not found in the string, return the original string\n",
      "    if last_occurrence_index == -1:\n",
      "        return input_string\n",
      "    \n",
      "    # Remove the first and last occurrence of the character\n",
      "    modified_string = input_string[:first_occurrence_index] + input_string[last_occurrence_index + 1:]\n",
      "    \n",
      "    return modified_string\n",
      "\n",
      "# Example usage:\n",
      "input_str = \"Hello, World!\"\n",
      "char_to_remove = \"l\"\n",
      "print(remove_first_and_last_occurrence(input_str, char_to_remove))  # Output: \"Heo, World!\"\n",
      "```\n",
      "\n",
      "This function uses the `find` method to find the index of the first occurrence of the character, and the `rfind` method to find the index of the last occurrence. It then uses slicing to remove the first and last occurrence of the character from the string. If the character is not found, it returns the original string.\n"
     ]
    }
   ],
   "source": [
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: 70\n"
     ]
    }
   ],
   "source": [
    "full_code = pred + \"\\n\" + \"\\n\".join(test_cases)\n",
    "\n",
    "# run the code\n",
    "try:\n",
    "    exec(full_code)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: 70\n",
      "<function max_record_value at 0x797d148c85e0>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m input_data, expected_output \u001b[38;5;241m=\u001b[39m parse_test_case(test_case)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(function_handle)\n\u001b[0;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m<string>:14\u001b[0m, in \u001b[0;36mmax_record_value\u001b[0;34m(tuples_list)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'str'"
     ]
    }
   ],
   "source": [
    "pred = extract_function(predicted_code)\n",
    "\n",
    "local_env = {}\n",
    "exec(pred, {}, local_env)\n",
    "function_handle = next(iter(local_env.values()))\n",
    "test_case = test_cases[0]\n",
    "input_data, expected_output = parse_test_case(test_case)\n",
    "print(function_handle)\n",
    "result = function_handle(eval(input_data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (f\"Problem: {problem_description}\\n\\n\"\"Input:\\nWrite a Python function to solve the problem above.\\nOutput:\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_code_block(predicted_code):\n",
    "    \"\"\"\n",
    "    Extracts the Python code block enclosed between ```python and ```.\n",
    "    \"\"\"\n",
    "    start_tag = \"```python\"\n",
    "    end_tag = \"```\"\n",
    "    \n",
    "    # Find the first Python code block\n",
    "    start_index = predicted_code.find(start_tag)\n",
    "    end_index = predicted_code.find(end_tag, start_index + len(start_tag))\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        # Extract and return the content inside the code block\n",
    "        return predicted_code[start_index + len(start_tag):end_index].strip()\n",
    "    \n",
    "    return \"No Python code block found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [('key1', 5), ('key2', 4), ('key3', 9)]\n",
      "result: [('key1', 6), ('key2', 5), ('key3', 10)]\n",
      "result: [('key1', 7), ('key2', 6), ('key3', 11)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_problem(function, test_cases):\n",
    "\n",
    "    success_count = 0\n",
    "    local_env = {}\n",
    "    exec(function, {}, local_env)\n",
    "    function_handle = next(iter(local_env.values()))\n",
    "    for test_case in test_cases:\n",
    "        input_data, expected_output = parse_test_case(test_case)\n",
    "        try:\n",
    "            result = function_handle(eval(input_data))\n",
    "            print(\"result:\", result)\n",
    "            if result == expected_output:\n",
    "                success_count += 1\n",
    "        except Exception as e:\n",
    "            print(\"not a python code\")\n",
    "            continue\n",
    "\n",
    "    # Return results\n",
    "    accuracy = success_count / len(test_cases)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "evaluate_problem(ground_truth_code, test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])]\", [('key1', 5), ('key2', 4), ('key3', 9)])\n",
      "(\"[('key1', [4, 5, 6]), ('key2', [2, 5, 3]), ('key3', [10, 4])]\", [('key1', 6), ('key2', 5), ('key3', 10)])\n",
      "(\"[('key1', [5, 6, 7]), ('key2', [3, 6, 4]), ('key3', [11, 5])]\", [('key1', 7), ('key2', 6), ('key3', 11)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def parse_test_case(test_case):\n",
    "    \"\"\"\n",
    "    Parse an assertion like:\n",
    "    assert max_chain_length([Pair(1, 2), Pair(3, 4)], 4) == 4\n",
    "    to extract:\n",
    "    - Function call as a string: \"max_chain_length([Pair(1, 2), Pair(3, 4)], 4)\"\n",
    "    - Expected output: 4\n",
    "    \"\"\"\n",
    "    # Parse the assertion string\n",
    "    assertion_node = ast.parse(test_case, mode='exec').body[0]\n",
    "    if isinstance(assertion_node, ast.Assert):\n",
    "        test_call = assertion_node.test.left  # The function call (left of ==)\n",
    "        # i want to take out the name of the function too\n",
    "        test_call = test_call.args\n",
    "        expected_output = assertion_node.test.comparators[0]  # The value (right of ==)\n",
    "\n",
    "        # Convert AST back to source code for evaluation\n",
    "        test_call_code = ast.unparse(test_call)\n",
    "        expected_output_value = eval(ast.unparse(expected_output))\n",
    "        return test_call_code, expected_output_value\n",
    "    else:\n",
    "        raise ValueError(\"Test case is not a valid assertion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def find_max_value_in_list(tuple_list):\n",
      "    \"\"\"\n",
      "    This function finds the maximum value in a list of tuples.\n",
      "\n",
      "    Parameters:\n",
      "    tuple_list (list): A list of tuples.\n",
      "\n",
      "    Returns:\n",
      "    tuple: The maximum value in the list of tuples.\n",
      "    \"\"\"\n",
      "    if not tuple_list:\n",
      "        return None\n",
      "    max_value = max(tuple_list, key=lambda x: x[1])\n",
      "    return max_value\n",
      "\n",
      "# Example usage:\n",
      "tuple_list = [(1, 2), (3, 4), (5, 6), (7, 8)]\n",
      "max_value = find_max_value_in_list(tuple_list)\n",
      "print(max_value)  # Output: (7, 8)\n",
      "(7, 8)\n",
      "not a python code\n",
      "(7, 8)\n",
      "not a python code\n",
      "(7, 8)\n",
      "not a python code\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "cleaned_code = extract_python_code_block(predicted_code)\n",
    "print(cleaned_code)\n",
    "accuracy = evaluate_problem(cleaned_code, test_cases)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"\"\"def find_max_value_in_list(tuple_list):\n",
    "    \\\"\\\"\\\"\n",
    "    This function finds the maximum value in a list of tuples.\n",
    "\n",
    "    Parameters:\n",
    "    tuple_list (list): A list of tuples.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The maximum value in the list of tuples.\n",
    "    \\\"\\\"\\\"\n",
    "    if not tuple_list:\n",
    "        return None\n",
    "    max_value = max(tuple_list, key=lambda x: x[1])\n",
    "    return max_value\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 12\n",
      "[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])] [('key1', 5), ('key2', 4), ('key3', 9)]\n"
     ]
    }
   ],
   "source": [
    "i = 12\n",
    "print(\"Problem\", i)\n",
    "sample_problem = mbpp['train'][i] \n",
    "problem_description = sample_problem[\"text\"]\n",
    "ground_truth_code = sample_problem[\"code\"]\n",
    "test_cases = sample_problem[\"test_list\"]\n",
    "input_data, expected_output = parse_test_case(test_cases[0])\n",
    "print(input_data, expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def maximum_value(test_list):\n",
      "  res = [(key, max(lst)) for key, lst in test_list]\n",
      "  return (res) \n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m local_env \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m exec(ground_truth_code, {}, local_env)\n\u001b[0;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_env\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m     12\u001b[0m exec(c, {}, local_env)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])]\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for test_case in test_cases:\n",
    "    input_data = '''[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])]'''\n",
    "    expected_output = \"[('key1', 5), ('key2', 4), ('key3', 9)]\"\n",
    "    #input_data, expected_output = parse_test_case(test_case)\n",
    "    # Use `exec` to run the code and check the output\n",
    "    local_env = {}\n",
    "\n",
    "    exec(ground_truth_code, {}, local_env)\n",
    "    result = local_env[input_data]\n",
    "    print(result)\n",
    "\n",
    "    exec(c, {}, local_env)\n",
    "    result = local_env[input_data]\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def maximum_value(test_list):\n",
      "  res = [(key, max(lst)) for key, lst in test_list]\n",
      "  return (res) \n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during function execution: not enough values to unpack (expected 2, got 1)\n"
     ]
    }
   ],
   "source": [
    "local_env = {}\n",
    "\n",
    "try:\n",
    "    # Dynamically execute the function string\n",
    "    exec(ground_truth_code, {}, local_env)\n",
    "\n",
    "    # Extract the function name (assumes it's the first 'def' in the string)\n",
    "    function_name = ground_truth_code.split(\"def \")[1].split(\"(\")[0].strip()\n",
    "\n",
    "    # Call the function dynamically with the input\n",
    "    result = local_env[function_name](input_data)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error during function execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(ground_truth_code, {}, local_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(ground_truth_code, {}, local_env)\n",
    "function_handle = next(iter(local_env.values()))\n",
    "result = function_handle(eval(input_data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key1', 5), ('key2', 4), ('key3', 9)]\n"
     ]
    }
   ],
   "source": [
    "result = function_handle(eval(input_data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function(predicted_code):\n",
    "    \"\"\"\n",
    "    Extracts a function from a code string starting with 'def' and ending based on indentation rules.\n",
    "    \"\"\"\n",
    "    lines = predicted_code.splitlines()  # Split code into lines\n",
    "    start_index = -1\n",
    "\n",
    "    # Find the first occurrence of a line starting with 'def'\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"def \"):  # Check for the 'def' keyword\n",
    "            start_index = i\n",
    "            break\n",
    "\n",
    "    if start_index == -1:\n",
    "        return \"No function definition found.\"\n",
    "\n",
    "    # Determine indentation of the function header\n",
    "    function_indent = len(lines[start_index]) - len(lines[start_index].lstrip())\n",
    "    function_lines = [lines[start_index]]  # Start collecting the function from 'def'\n",
    "\n",
    "    # Collect all indented lines after the function header\n",
    "    for line in lines[start_index + 1:]:\n",
    "        current_indent = len(line) - len(line.lstrip())\n",
    "\n",
    "        # Stop if indentation decreases or line is blank and isn't a continuation\n",
    "        if current_indent <= function_indent:\n",
    "            break\n",
    "\n",
    "        function_lines.append(line)\n",
    "\n",
    "    # Join the collected lines to reconstruct the function\n",
    "    return \"\\n\".join(function_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 5), ('key2', 4), ('key3', 9)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maximum_value(test_list):\n",
    "  res = [(key, max(lst)) for key, lst in test_list]\n",
    "  return (res) \n",
    "\n",
    "maximum_value([(\"key1\", [3, 4, 5]), (\"key2\", [1, 4, 2]), (\"key3\", [9, 3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a single Python function to solve the problem above.\n",
      "\n",
      "```python\n",
      "def find_max_value_in_list(tuple_list):\n",
      "    \"\"\"\n",
      "    This function finds the maximum value in a list of tuples.\n",
      "\n",
      "    Parameters:\n",
      "    tuple_list (list): A list of tuples.\n",
      "\n",
      "    Returns:\n",
      "    tuple: The maximum value in the list of tuples.\n",
      "    \"\"\"\n",
      "    if not tuple_list:\n",
      "        return None\n",
      "    max_value = max(tuple_list, key=lambda x: x[1])\n",
      "    return max_value\n",
      "\n",
      "# Example usage:\n",
      "tuple_list = [(1, 2), (3, 4), (5, 6), (7, 8)]\n",
      "max_value = find_max_value_in_list(tuple_list)\n",
      "print(max_value)  # Output: (7, 8)\n",
      "```\n",
      "\n",
      "This solution works by using the built-in `max` function in Python, which returns the largest item in an iterable. The `key` argument of the `max` function is used to specify that we want to compare the tuples based on their values (i.e., the second element of each tuple). The `max_value` variable is assigned the maximum tuple found in the list. Finally, the function returns `max_value`. The example usage demonstrates how to call the function with a list of tuples and print the result.\n"
     ]
    }
   ],
   "source": [
    "print(predicted_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
