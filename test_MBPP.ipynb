{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mbpp = load_dataset(\"mbpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 11,\n",
       " 'text': 'Write a python function to remove first and last occurrence of a given character from the string.',\n",
       " 'code': 'def remove_Occ(s,ch): \\r\\n    for i in range(len(s)): \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    for i in range(len(s) - 1,-1,-1):  \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    return s ',\n",
       " 'test_list': ['assert remove_Occ(\"hello\",\"l\") == \"heo\"',\n",
       "  'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"',\n",
       "  'assert remove_Occ(\"PHP\",\"P\") == \"H\"'],\n",
       " 'test_setup_code': '',\n",
       " 'challenge_test_list': ['assert remove_Occ(\"hellolloll\",\"l\") == \"helollol\"',\n",
       "  'assert remove_Occ(\"\",\"l\") == \"\"']}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbpp['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "def load_model_and_tokenizer(model_name = \"google/gemma-2b-it\", device = None):\n",
    "    \n",
    "    if device is None:\n",
    "      device = 'cpu'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                              trust_remote_code=True,\n",
    "                                              )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device, torch_dtype=torch.bfloat16)\n",
    "    model.eval()\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer('meta-llama/Llama-3.2-1B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def predict_code(model, tokenizer, problem_description):\n",
    "\n",
    "    prompt = [{\"role\":\"user\", \"content\": f\"Problem: {problem_description}\\n\\n\"\n",
    "            \"Input:\\nWrite a single Python function to solve the problem above.\\nOutput:\\n\"}] \n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    print(outputs)\n",
    "    predicted_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return predicted_code[len(prompt):]\n",
    "\n",
    "def parse_test_case(test_case):\n",
    "    \"\"\"\n",
    "    Parse an assertion like:\n",
    "    assert max_chain_length([Pair(1, 2), Pair(3, 4)], 4) == 4\n",
    "    to extract:\n",
    "    - Function call as a string: \"max_chain_length([Pair(1, 2), Pair(3, 4)], 4)\"\n",
    "    - Expected output: 4\n",
    "    \"\"\"\n",
    "    # Parse the assertion string\n",
    "    assertion_node = ast.parse(test_case, mode='exec').body[0]\n",
    "    if isinstance(assertion_node, ast.Assert):\n",
    "        test_call = assertion_node.test.left  # The function call (left of ==)\n",
    "        # i want to take out the name of the function too\n",
    "        test_call = test_call.args\n",
    "        expected_output = assertion_node.test.comparators[0]  # The value (right of ==)\n",
    "\n",
    "        # Convert AST back to source code for evaluation\n",
    "        test_call_code = ast.unparse(test_call)\n",
    "        expected_output_value = eval(ast.unparse(expected_output))\n",
    "        return test_call_code, expected_output_value\n",
    "    else:\n",
    "        raise ValueError(\"Test case is not a valid assertion.\")\n",
    "    \n",
    "\n",
    "def evaluate_problem(function, test_cases):\n",
    "\n",
    "    success_count = 0\n",
    "    local_env = {}\n",
    "    exec(function, {}, local_env)\n",
    "    function_handle = next(iter(local_env.values()))\n",
    "    for test_case in test_cases:\n",
    "        input_data, expected_output = parse_test_case(test_case)\n",
    "        try:\n",
    "            result = function_handle(eval(input_data))\n",
    "            if result == expected_output:\n",
    "                success_count += 1\n",
    "        except Exception as e:\n",
    "            print(\"not a python code\")\n",
    "            continue\n",
    "\n",
    "    # Return results\n",
    "    accuracy = success_count / len(test_cases)\n",
    "    print(accuracy)\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m ground_truth_code \u001b[38;5;241m=\u001b[39m sample_problem[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m test_cases \u001b[38;5;241m=\u001b[39m sample_problem[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m predicted_code \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_code)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ground_truth_code)\n",
      "Cell \u001b[0;32mIn[93], line 10\u001b[0m, in \u001b[0;36mpredict_code\u001b[0;34m(model, tokenizer, problem_description)\u001b[0m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblem_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWrite a single Python function to solve the problem above.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}] \n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     12\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     13\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     15\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     16\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m predicted_code \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_code[\u001b[38;5;28mlen\u001b[39m(prompt):]\n",
      "\u001b[0;31mTypeError\u001b[0m: transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "source": [
    "def clean_code(predicted_code):\n",
    "    \"\"\"\n",
    "    Finds the first occurrence of 'def' in the predicted code and removes everything before it.\n",
    "    If 'def' is not found, returns the code unchanged.\n",
    "    \"\"\"\n",
    "    def_index = predicted_code.find(\"def\")  # Find the index of the first 'def'\n",
    "    if def_index != -1:\n",
    "        return predicted_code[def_index:]  # Return the substring starting from 'def'\n",
    "    return predicted_code  \n",
    "for i in range(0, 10):\n",
    "    print(\"Problem\", i)\n",
    "    sample_problem = mbpp['train'][i] \n",
    "    problem_description = sample_problem[\"text\"]\n",
    "    ground_truth_code = sample_problem[\"code\"]\n",
    "    test_cases = sample_problem[\"test_list\"]\n",
    "\n",
    "    predicted_code = predict_code(model, tokenizer, problem_description)\n",
    "    print(\"pred:\", predicted_code)\n",
    "    print(\"ground:\", ground_truth_code)\n",
    "    cleaned_code = clean_code(predicted_code)\n",
    "    print(\"cleaned:\", cleaned_code)\n",
    "    accuracy = evaluate_problem(cleaned_code, test_cases)\n",
    "    print(accuracy)\n",
    "# sample_problem = mbpp['train'][12] \n",
    "# problem_description = sample_problem[\"text\"]\n",
    "# ground_truth_code = sample_problem[\"code\"]\n",
    "# test_cases = sample_problem[\"test_list\"]\n",
    "\n",
    "\n",
    "# predicted_code = predict_code(model, tokenizer, problem_description)\n",
    "# accuracy = evaluate_problem(predicted_code, test_cases)\n",
    "# print(f\"Problem Description:\\n{problem_description}\")\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "# print(f\"Predicted Code:\\n{predicted_code}\")\n",
    "# print(f\"Ground Truth Code:\\n{ground_truth_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 12\n",
      "cleaned:\n",
      "\n",
      " def max_record_value(tuples_list):\n",
      "    \"\"\"\n",
      "    This function finds the maximum value in a list of tuples.\n",
      "\n",
      "    Args:\n",
      "    tuples_list (list): A list of tuples.\n",
      "\n",
      "    Returns:\n",
      "    tuple: The maximum value in the list of tuples.\n",
      "    \"\"\"\n",
      "    if not tuples_list:\n",
      "        return None  # Return None if the list is empty\n",
      "\n",
      "    max_value = max(tuples_list[0])  # Initialize max_value with the first tuple\n",
      "    for tuple in tuples_list[1:]:  # Iterate over the rest of the tuples\n",
      "        if tuple[0] > max_value:  # Compare the first element of the current tuple with max_value\n",
      "            max_value = tuple[0]  # Update max_value if the current tuple's first element is larger\n",
      "\n",
      "    return max_value  # Return the maximum value\n",
      "\n",
      "\n",
      "# Example usage\n",
      "tuples_list = [(10, 20, 30), (40, 50, 60), (70, 80, 90)]\n",
      "max_value = max_record_value(tuples_list)\n",
      "print(\"Maximum value:\", max_value)  # Output: Maximum value: (90, 60, 70)\n",
      "1.0\n",
      "v\n",
      "Maximum value: 70\n",
      "not a python code\n",
      "not a python code\n",
      "not a python code\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "i = 12\n",
    "print(\"Problem\", i)\n",
    "sample_problem = mbpp['train'][i] \n",
    "problem_description = sample_problem[\"text\"]\n",
    "ground_truth_code = sample_problem[\"code\"]\n",
    "test_cases = sample_problem[\"test_list\"]\n",
    "\n",
    "# predicted_code = predict_code(model, tokenizer, problem_description)\n",
    "# print(\"pred:\", predicted_code)\n",
    "# print(\"ground:\", ground_truth_code)\n",
    "\n",
    "# extract code inside ```python\n",
    "import re\n",
    "def extract_function(predicted_code):\n",
    "    pattern = r\"```python\\n(.*)\\n```\"\n",
    "    match = re.findall(pattern, predicted_code, re.DOTALL)\n",
    "    return match[-1]\n",
    "\n",
    "print(\"cleaned:\\n\\n\", extract_function(predicted_code))\n",
    "\n",
    "#execute the code\n",
    "#result = evaluate_problem(ground_truth_code, test_cases)\n",
    "#print(\"v\")\n",
    "#result = evaluate_problem(extract_function(predicted_code), test_cases)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# cleaned_code = extract_function(predicted_code)\n",
    "# print(\"cleaned:\", cleaned_code)\n",
    "# accuracy = evaluate_problem(cleaned_code, test_cases)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def max_record_value(tuples_list):\n",
      "    \"\"\"\n",
      "    This function finds the maximum value in a list of tuples.\n",
      "\n",
      "    Args:\n",
      "    tuples_list (list): A list of tuples.\n",
      "\n",
      "    Returns:\n",
      "    tuple: The maximum value in the list of tuples.\n",
      "    \"\"\"\n",
      "    if not tuples_list:\n",
      "        return None  # Return None if the list is empty\n",
      "\n",
      "    max_value = max(tuples_list[0])  # Initialize max_value with the first tuple\n",
      "    for tuple in tuples_list[1:]:  # Iterate over the rest of the tuples\n",
      "        if tuple[0] > max_value:  # Compare the first element of the current tuple with max_value\n",
      "            max_value = tuple[0]  # Update max_value if the current tuple's first element is larger\n",
      "\n",
      "    return max_value  # Return the maximum value\n",
      "\n",
      "\n",
      "# Example usage\n",
      "tuples_list = [(10, 20, 30), (40, 50, 60), (70, 80, 90)]\n",
      "max_value = max_record_value(tuples_list)\n",
      "print(\"Maximum value:\", max_value)  # Output: Maximum value: (90, 60, 70)\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "import re\n",
    "\n",
    "def extract_python(predicted_code):\n",
    "    pattern = r\"```python\\n(.*)\\n```\"\n",
    "    match = re.findall(pattern, predicted_code, re.DOTALL)\n",
    "    return match[-1]\n",
    "\n",
    "def run_mbpp_benchmark(model, prompt_type, max_tokens=1024):\n",
    "    import datasets\n",
    "    data = datasets.load_dataset(\"mbpp\")['test']\n",
    "    \n",
    "    inputs = []\n",
    "    test_cases = []\n",
    "    for i in range(len(data)):\n",
    "        inputs.append([{ \"role\": \"user\", \"content\": data[i]['text']}])\n",
    "        test_cases.append(\"\\n\".join(data[i]['challenge_test_list']))\n",
    "\n",
    "    if prompt_type == 'chat':\n",
    "        inputs = model.get_tokenizer().apply_chat_template(inputs, add_generation_prompt=True, tokenize=False)\n",
    "    elif prompt_type == 'plain':\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError('problem_prompt should be either \"chat\" or \"plain\"')\n",
    "    \n",
    "\n",
    "    sampling = SamplingParams(n=1, temperature=0.0, max_tokens=max_tokens, top_p=1.0)\n",
    "    \n",
    "    results = model.generate(inputs, sampling_params=sampling)\n",
    "    results = extract_outputs(results)\n",
    "\n",
    "    accuracy = 0\n",
    "    for i in range(len(results)):\n",
    "        predicted_code = results[i]\n",
    "        cleaned_code = extract_python(predicted_code)\n",
    "        try:\n",
    "            exec(cleaned_code+'\\n'+test_cases[i])\n",
    "            accuracy += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return accuracy / len(results)\n",
    "\n",
    "def extract_outputs(outputs):\n",
    "    return [output.outputs[0].text for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 22:24:24 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 12-06 22:24:24 config.py:1020] Defaulting to use mp for distributed inference\n",
      "WARNING 12-06 22:24:24 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-06 22:24:24 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-06 22:24:24 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=39808, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "WARNING 12-06 22:24:24 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 36 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-06 22:24:24 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 12-06 22:24:25 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:25 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:25 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-06 22:24:25 utils.py:961] Found nccl from library libnccl.so.2\n",
      "ERROR 12-06 22:24:25 pynccl_wrapper.py:196] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise, the nccl library might not exist, be corrupted or it does not support the current platform Linux-6.5.0-35-generic-x86_64-with-glibc2.35.If you already have the library, please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:25 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:25 pynccl_wrapper.py:196] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise, the nccl library might not exist, be corrupted or it does not support the current platform Linux-6.5.0-35-generic-x86_64-with-glibc2.35.If you already have the library, please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\n",
      "INFO 12-06 22:24:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/amin/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 12-06 22:24:25 custom_all_reduce.py:143] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/amin/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m WARNING 12-06 22:24:25 custom_all_reduce.py:143] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-06 22:24:25 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x72ab22655880>, local_subscribe_port=53171, remote_subscribe_port=None)\n",
      "INFO 12-06 22:24:25 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:25 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "INFO 12-06 22:24:25 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:25 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 12-06 22:24:25 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244243c3e4b54fc6970d96dd2237315e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:26 weight_utils.py:288] No model.safetensors.index.json found in remote.\n",
      "INFO 12-06 22:24:26 model_runner.py:1077] Loading model weights took 1.1666 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:26 model_runner.py:1077] Loading model weights took 1.1666 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:26 worker.py:232] Memory profiling results: total_gpu_memory=23.53GiB initial_memory_usage=7.40GiB peak_torch_memory=1.21GiB memory_usage_post_profile=7.60GiB non_torch_memory=6.42GiB kv_cache_size=13.55GiB gpu_memory_utilization=0.90\n",
      "INFO 12-06 22:24:27 worker.py:232] Memory profiling results: total_gpu_memory=23.54GiB initial_memory_usage=16.61GiB peak_torch_memory=2.34GiB memory_usage_post_profile=16.81GiB non_torch_memory=15.64GiB kv_cache_size=3.21GiB gpu_memory_utilization=0.90\n",
      "INFO 12-06 22:24:27 distributed_gpu_executor.py:57] # GPU blocks: 13153, # CPU blocks: 16384\n",
      "INFO 12-06 22:24:27 distributed_gpu_executor.py:61] Maximum concurrency for 39808 tokens per request: 5.29x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:28 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m INFO 12-06 22:24:28 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-06 22:24:28 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-06 22:24:28 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229] Exception in worker VllmWorkerProcess while processing method initialize_cache.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/worker.py\", line 277, in initialize_cache\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     self._warm_up_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/worker.py\", line 293, in _warm_up_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     self.model_runner.capture_model(self.gpu_cache)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1508, in capture_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     graph_runner.capture(**capture_inputs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1775, in capture\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     self.model(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 553, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     model_output = self.model(input_ids, positions, kv_caches,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 143, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     return self.forward(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 331, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     hidden_states = self.get_input_embeddings(input_ids)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 316, in get_input_embeddings\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     return self.embed_tokens(input_ids)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 419, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     output = tensor_model_parallel_all_reduce(output_parallel)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/communication_op.py\", line 11, in tensor_model_parallel_all_reduce\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     return get_tp_group().all_reduce(input_)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py\", line 394, in all_reduce\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     torch.ops.vllm.inplace_all_reduce(input_,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/_ops.py\", line 1116, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     return self._op(*args, **(kwargs or {}))\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py\", line 106, in inplace_all_reduce\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     group._all_reduce_in_place(tensor)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py\", line 409, in _all_reduce_in_place\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     pynccl_comm.all_reduce(input_)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl.py\", line 123, in all_reduce\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]     assert tensor.device == self.device, (\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229]                             ^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=171550)\u001b[0;0m ERROR 12-06 22:24:28 multiproc_worker_utils.py:229] AttributeError: 'PyNcclCommunicator' object has no attribute 'device'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PyNcclCommunicator' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mEvalUtils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-1B-Instruct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP_Mixture/EvalUtils/_ModelUtils.py:11\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_model_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m39808\u001b[39m \u001b[38;5;66;03m## This is 4090 Specific Limit\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/utils.py:1028\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1025\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m         )\n\u001b[0;32m-> 1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/entrypoints/llm.py:210\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/engine/llm_engine.py:585\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    583\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/engine/llm_engine.py:350\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config, )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/engine/llm_engine.py:500\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/executor/distributed_gpu_executor.py:67\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minitialize_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/executor/multiproc_gpu_executor.py:195\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m worker_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    190\u001b[0m     worker\u001b[38;5;241m.\u001b[39mexecute_method(method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m    192\u001b[0m ]\n\u001b[1;32m    194\u001b[0m driver_worker_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, method)\n\u001b[0;32m--> 195\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[43mdriver_worker_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[1;32m    199\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/worker.py:277\u001b[0m, in \u001b[0;36mWorker.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_cache_engine()\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_warm_up_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/worker.py:293\u001b[0m, in \u001b[0;36mWorker._warm_up_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_warm_up_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39menforce_eager:\n\u001b[0;32m--> 293\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# Reset the seed to ensure that the random state is not affected by\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# the model initialization and profiling.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     set_random_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mseed)\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/model_runner.py:1508\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.capture_model\u001b[0;34m(self, kv_caches)\u001b[0m\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inputs_to_capture_for_enc_dec_model(\n\u001b[1;32m   1505\u001b[0m         capture_inputs)\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_forward_context(attn_metadata):\n\u001b[0;32m-> 1508\u001b[0m     \u001b[43mgraph_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcapture_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_memory_pool \u001b[38;5;241m=\u001b[39m graph_runner\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mpool()\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_runners[virtual_engine][batch_size] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1511\u001b[0m     graph_runner)\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/model_runner.py:1775\u001b[0m, in \u001b[0;36mCUDAGraphRunner.capture\u001b[0;34m(self, input_ids, positions, intermediate_inputs, kv_caches, attn_metadata, memory_pool, stream, **kwargs)\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;66;03m# Run the model a few times without capturing the graph.\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;66;03m# This is to make sure that the captured graph does not include the\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;66;03m# kernel launches for initial benchmarking (e.g., Triton autotune).\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;66;03m# Note one iteration is not enough for torch.jit.script\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(_NUM_WARMUP_ITERS):\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# Wait for the warm up operations to finish before proceeding with\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;66;03m# Graph Capture.\u001b[39;00m\n\u001b[1;32m   1785\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:553\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    546\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    552\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 553\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/compilation/decorators.py:143\u001b[0m, in \u001b[0;36m_support_torch_compile.<locals>.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# torch.compiler.is_compiling() means we are inside the compilation\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# e.g. TPU has the compilation logic in model runner, so we don't\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# need to compile the model inside.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_compile \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling():\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# the first compilation needs to have dynamic shapes marked\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_codes) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:331\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    329\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:316\u001b[0m, in \u001b[0;36mLlamaModel.get_input_embeddings\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py:419\u001b[0m, in \u001b[0;36mVocabParallelEmbedding.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    417\u001b[0m     output_parallel\u001b[38;5;241m.\u001b[39mmasked_fill_(input_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Reduce across all the model parallel GPUs.\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_model_parallel_all_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_parallel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/communication_op.py:11\u001b[0m, in \u001b[0;36mtensor_model_parallel_all_reduce\u001b[0;34m(input_)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor_model_parallel_all_reduce\u001b[39m(input_: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"All-reduce the input tensor across model parallel group.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tp_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:394\u001b[0m, in \u001b[0;36mGroupCoordinator.all_reduce\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mvllm\u001b[38;5;241m.\u001b[39moutplace_all_reduce(\n\u001b[1;32m    392\u001b[0m         input_, group_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_name)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_all_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m input_\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:106\u001b[0m, in \u001b[0;36minplace_all_reduce\u001b[0;34m(tensor, group_name)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is destroyed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m \u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_all_reduce_in_place\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:409\u001b[0m, in \u001b[0;36mGroupCoordinator._all_reduce_in_place\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    407\u001b[0m pynccl_comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpynccl_comm\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (pynccl_comm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pynccl_comm\u001b[38;5;241m.\u001b[39mdisabled):\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mpynccl_comm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mall_reduce(input_, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_group)\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl.py:123\u001b[0m, in \u001b[0;36mPyNcclCommunicator.all_reduce\u001b[0;34m(self, tensor, op, stream)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# nccl communicator created on a specific device\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# will only work on tensors on the same device\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# otherwise it will cause \"illegal memory access\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m, (\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis nccl communicator is created to work on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the input tensor is on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PyNcclCommunicator' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "from EvalUtils import load_model\n",
    "\n",
    "model = load_model('meta-llama/Llama-3.2-1B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 22:22:52 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 12-06 22:22:52 config.py:1020] Defaulting to use mp for distributed inference\n",
      "WARNING 12-06 22:22:52 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-06 22:22:52 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-06 22:22:52 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=39808, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "WARNING 12-06 22:22:53 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 36 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-06 22:22:53 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 12-06 22:22:53 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:22:53 selector.py:135] Using Flash Attention backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:22:53 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-06 22:22:54 utils.py:961] Found nccl from library libnccl.so.2\n",
      "ERROR 12-06 22:22:54 pynccl_wrapper.py:196] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise, the nccl library might not exist, be corrupted or it does not support the current platform Linux-6.5.0-35-generic-x86_64-with-glibc2.35.If you already have the library, please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:22:54 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:22:54 pynccl_wrapper.py:196] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise, the nccl library might not exist, be corrupted or it does not support the current platform Linux-6.5.0-35-generic-x86_64-with-glibc2.35.If you already have the library, please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.\n",
      "INFO 12-06 22:22:54 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /home/amin/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 12-06 22:23:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/amin/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 12-06 22:23:02 custom_all_reduce.py:143] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:23:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/amin/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m WARNING 12-06 22:23:02 custom_all_reduce.py:143] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-06 22:23:02 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7980f8c1e210>, local_subscribe_port=41411, remote_subscribe_port=None)\n",
      "INFO 12-06 22:23:02 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:23:02 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "INFO 12-06 22:23:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:23:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 12-06 22:23:03 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a03e37e45eb4987b4da291342a612f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:23:03 weight_utils.py:288] No model.safetensors.index.json found in remote.\n",
      "INFO 12-06 22:23:03 model_runner.py:1077] Loading model weights took 1.1666 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:23:03 model_runner.py:1077] Loading model weights took 1.1666 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:23:04 worker.py:232] Memory profiling results: total_gpu_memory=23.53GiB initial_memory_usage=7.40GiB peak_torch_memory=1.21GiB memory_usage_post_profile=7.60GiB non_torch_memory=6.42GiB kv_cache_size=13.55GiB gpu_memory_utilization=0.90\n",
      "INFO 12-06 22:23:04 worker.py:232] Memory profiling results: total_gpu_memory=23.54GiB initial_memory_usage=16.61GiB peak_torch_memory=2.34GiB memory_usage_post_profile=16.81GiB non_torch_memory=15.64GiB kv_cache_size=3.21GiB gpu_memory_utilization=0.90\n",
      "INFO 12-06 22:23:04 distributed_gpu_executor.py:57] # GPU blocks: 13153, # CPU blocks: 16384\n",
      "INFO 12-06 22:23:04 distributed_gpu_executor.py:61] Maximum concurrency for 39808 tokens per request: 5.29x\n",
      "INFO 12-06 22:23:05 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-06 22:23:05 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:23:06 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m INFO 12-06 22:23:06 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229] Exception in worker VllmWorkerProcess while processing method initialize_cache.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/worker.py\", line 277, in initialize_cache\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     self._warm_up_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/worker.py\", line 293, in _warm_up_model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PyNcclCommunicator' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mEvalUtils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-1B-Instruct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m run_mbpp_benchmark(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m'\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n",
      "File \u001b[0;32m~/NLP_Mixture/EvalUtils/_ModelUtils.py:11\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_model_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m39808\u001b[39m \u001b[38;5;66;03m## This is 4090 Specific Limit\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/utils.py:1028\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1025\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m         )\n\u001b[0;32m-> 1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/entrypoints/llm.py:210\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/engine/llm_engine.py:585\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    583\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/engine/llm_engine.py:350\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config, )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/engine/llm_engine.py:500\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/executor/distributed_gpu_executor.py:67\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minitialize_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/executor/multiproc_gpu_executor.py:195\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m worker_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    190\u001b[0m     worker\u001b[38;5;241m.\u001b[39mexecute_method(method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m    192\u001b[0m ]\n\u001b[1;32m    194\u001b[0m driver_worker_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, method)\n\u001b[0;32m--> 195\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[43mdriver_worker_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[1;32m    199\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/worker.py:277\u001b[0m, in \u001b[0;36mWorker.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_cache_engine()\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_warm_up_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/worker.py:293\u001b[0m, in \u001b[0;36mWorker._warm_up_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_warm_up_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39menforce_eager:\n\u001b[0;32m--> 293\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# Reset the seed to ensure that the random state is not affected by\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# the model initialization and profiling.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     set_random_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mseed)\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/model_runner.py:1508\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.capture_model\u001b[0;34m(self, kv_caches)\u001b[0m\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inputs_to_capture_for_enc_dec_model(\n\u001b[1;32m   1505\u001b[0m         capture_inputs)\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_forward_context(attn_metadata):\n\u001b[0;32m-> 1508\u001b[0m     \u001b[43mgraph_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcapture_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_memory_pool \u001b[38;5;241m=\u001b[39m graph_runner\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mpool()\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_runners[virtual_engine][batch_size] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1511\u001b[0m     graph_runner)\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/model_runner.py:1775\u001b[0m, in \u001b[0;36mCUDAGraphRunner.capture\u001b[0;34m(self, input_ids, positions, intermediate_inputs, kv_caches, attn_metadata, memory_pool, stream, **kwargs)\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;66;03m# Run the model a few times without capturing the graph.\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;66;03m# This is to make sure that the captured graph does not include the\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;66;03m# kernel launches for initial benchmarking (e.g., Triton autotune).\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;66;03m# Note one iteration is not enough for torch.jit.script\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(_NUM_WARMUP_ITERS):\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# Wait for the warm up operations to finish before proceeding with\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;66;03m# Graph Capture.\u001b[39;00m\n\u001b[1;32m   1785\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:553\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    546\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    552\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 553\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/compilation/decorators.py:143\u001b[0m, in \u001b[0;36m_support_torch_compile.<locals>.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# torch.compiler.is_compiling() means we are inside the compilation\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# e.g. TPU has the compilation logic in model runner, so we don't\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# need to compile the model inside.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_compile \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling():\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# the first compilation needs to have dynamic shapes marked\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_codes) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:331\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    329\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:316\u001b[0m, in \u001b[0;36mLlamaModel.get_input_embeddings\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py:419\u001b[0m, in \u001b[0;36mVocabParallelEmbedding.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    417\u001b[0m     output_parallel\u001b[38;5;241m.\u001b[39mmasked_fill_(input_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Reduce across all the model parallel GPUs.\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_model_parallel_all_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_parallel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/communication_op.py:11\u001b[0m, in \u001b[0;36mtensor_model_parallel_all_reduce\u001b[0;34m(input_)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor_model_parallel_all_reduce\u001b[39m(input_: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"All-reduce the input tensor across model parallel group.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tp_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:394\u001b[0m, in \u001b[0;36mGroupCoordinator.all_reduce\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mvllm\u001b[38;5;241m.\u001b[39moutplace_all_reduce(\n\u001b[1;32m    392\u001b[0m         input_, group_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_name)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_all_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m input_\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:106\u001b[0m, in \u001b[0;36minplace_all_reduce\u001b[0;34m(tensor, group_name)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is destroyed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m \u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_all_reduce_in_place\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:409\u001b[0m, in \u001b[0;36mGroupCoordinator._all_reduce_in_place\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    407\u001b[0m pynccl_comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpynccl_comm\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (pynccl_comm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pynccl_comm\u001b[38;5;241m.\u001b[39mdisabled):\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mpynccl_comm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mall_reduce(input_, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_group)\n",
      "File \u001b[0;32m~/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl.py:123\u001b[0m, in \u001b[0;36mPyNcclCommunicator.all_reduce\u001b[0;34m(self, tensor, op, stream)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# nccl communicator created on a specific device\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# will only work on tensors on the same device\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# otherwise it will cause \"illegal memory access\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m, (\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis nccl communicator is created to work on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the input tensor is on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PyNcclCommunicator' object has no attribute 'device'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     self.model_runner.capture_model(self.gpu_cache)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1508, in capture_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     graph_runner.capture(**capture_inputs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1775, in capture\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     self.model(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 553, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     model_output = self.model(input_ids, positions, kv_caches,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 143, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     return self.forward(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 331, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     hidden_states = self.get_input_embeddings(input_ids)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 316, in get_input_embeddings\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     return self.embed_tokens(input_ids)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 419, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     output = tensor_model_parallel_all_reduce(output_parallel)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/communication_op.py\", line 11, in tensor_model_parallel_all_reduce\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     return get_tp_group().all_reduce(input_)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py\", line 394, in all_reduce\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     torch.ops.vllm.inplace_all_reduce(input_,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/torch/_ops.py\", line 1116, in __call__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     return self._op(*args, **(kwargs or {}))\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py\", line 106, in inplace_all_reduce\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     group._all_reduce_in_place(tensor)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/parallel_state.py\", line 409, in _all_reduce_in_place\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     pynccl_comm.all_reduce(input_)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]   File \"/home/amin/miniforge3/envs/NLP/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl.py\", line 123, in all_reduce\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]     assert tensor.device == self.device, (\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229]                             ^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=170833)\u001b[0;0m ERROR 12-06 22:23:06 multiproc_worker_utils.py:229] AttributeError: 'PyNcclCommunicator' object has no attribute 'device'\n"
     ]
    }
   ],
   "source": [
    "run_mbpp_benchmark(model, 'chat', max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: 70\n"
     ]
    }
   ],
   "source": [
    "full_code = pred + \"\\n\" + \"\\n\".join(test_cases)\n",
    "\n",
    "# run the code\n",
    "try:\n",
    "    exec(full_code)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: 70\n",
      "<function max_record_value at 0x797d148c85e0>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m input_data, expected_output \u001b[38;5;241m=\u001b[39m parse_test_case(test_case)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(function_handle)\n\u001b[0;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m<string>:14\u001b[0m, in \u001b[0;36mmax_record_value\u001b[0;34m(tuples_list)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'str'"
     ]
    }
   ],
   "source": [
    "pred = extract_function(predicted_code)\n",
    "\n",
    "local_env = {}\n",
    "exec(pred, {}, local_env)\n",
    "function_handle = next(iter(local_env.values()))\n",
    "test_case = test_cases[0]\n",
    "input_data, expected_output = parse_test_case(test_case)\n",
    "print(function_handle)\n",
    "result = function_handle(eval(input_data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (f\"Problem: {problem_description}\\n\\n\"\"Input:\\nWrite a Python function to solve the problem above.\\nOutput:\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_code_block(predicted_code):\n",
    "    \"\"\"\n",
    "    Extracts the Python code block enclosed between ```python and ```.\n",
    "    \"\"\"\n",
    "    start_tag = \"```python\"\n",
    "    end_tag = \"```\"\n",
    "    \n",
    "    # Find the first Python code block\n",
    "    start_index = predicted_code.find(start_tag)\n",
    "    end_index = predicted_code.find(end_tag, start_index + len(start_tag))\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        # Extract and return the content inside the code block\n",
    "        return predicted_code[start_index + len(start_tag):end_index].strip()\n",
    "    \n",
    "    return \"No Python code block found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [('key1', 5), ('key2', 4), ('key3', 9)]\n",
      "result: [('key1', 6), ('key2', 5), ('key3', 10)]\n",
      "result: [('key1', 7), ('key2', 6), ('key3', 11)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_problem(function, test_cases):\n",
    "\n",
    "    success_count = 0\n",
    "    local_env = {}\n",
    "    exec(function, {}, local_env)\n",
    "    function_handle = next(iter(local_env.values()))\n",
    "    for test_case in test_cases:\n",
    "        input_data, expected_output = parse_test_case(test_case)\n",
    "        try:\n",
    "            result = function_handle(eval(input_data))\n",
    "            print(\"result:\", result)\n",
    "            if result == expected_output:\n",
    "                success_count += 1\n",
    "        except Exception as e:\n",
    "            print(\"not a python code\")\n",
    "            continue\n",
    "\n",
    "    # Return results\n",
    "    accuracy = success_count / len(test_cases)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "evaluate_problem(ground_truth_code, test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])]\", [('key1', 5), ('key2', 4), ('key3', 9)])\n",
      "(\"[('key1', [4, 5, 6]), ('key2', [2, 5, 3]), ('key3', [10, 4])]\", [('key1', 6), ('key2', 5), ('key3', 10)])\n",
      "(\"[('key1', [5, 6, 7]), ('key2', [3, 6, 4]), ('key3', [11, 5])]\", [('key1', 7), ('key2', 6), ('key3', 11)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def parse_test_case(test_case):\n",
    "    \"\"\"\n",
    "    Parse an assertion like:\n",
    "    assert max_chain_length([Pair(1, 2), Pair(3, 4)], 4) == 4\n",
    "    to extract:\n",
    "    - Function call as a string: \"max_chain_length([Pair(1, 2), Pair(3, 4)], 4)\"\n",
    "    - Expected output: 4\n",
    "    \"\"\"\n",
    "    # Parse the assertion string\n",
    "    assertion_node = ast.parse(test_case, mode='exec').body[0]\n",
    "    if isinstance(assertion_node, ast.Assert):\n",
    "        test_call = assertion_node.test.left  # The function call (left of ==)\n",
    "        # i want to take out the name of the function too\n",
    "        test_call = test_call.args\n",
    "        expected_output = assertion_node.test.comparators[0]  # The value (right of ==)\n",
    "\n",
    "        # Convert AST back to source code for evaluation\n",
    "        test_call_code = ast.unparse(test_call)\n",
    "        expected_output_value = eval(ast.unparse(expected_output))\n",
    "        return test_call_code, expected_output_value\n",
    "    else:\n",
    "        raise ValueError(\"Test case is not a valid assertion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def find_max_value_in_list(tuple_list):\n",
      "    \"\"\"\n",
      "    This function finds the maximum value in a list of tuples.\n",
      "\n",
      "    Parameters:\n",
      "    tuple_list (list): A list of tuples.\n",
      "\n",
      "    Returns:\n",
      "    tuple: The maximum value in the list of tuples.\n",
      "    \"\"\"\n",
      "    if not tuple_list:\n",
      "        return None\n",
      "    max_value = max(tuple_list, key=lambda x: x[1])\n",
      "    return max_value\n",
      "\n",
      "# Example usage:\n",
      "tuple_list = [(1, 2), (3, 4), (5, 6), (7, 8)]\n",
      "max_value = find_max_value_in_list(tuple_list)\n",
      "print(max_value)  # Output: (7, 8)\n",
      "(7, 8)\n",
      "not a python code\n",
      "(7, 8)\n",
      "not a python code\n",
      "(7, 8)\n",
      "not a python code\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "cleaned_code = extract_python_code_block(predicted_code)\n",
    "print(cleaned_code)\n",
    "accuracy = evaluate_problem(cleaned_code, test_cases)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"\"\"def find_max_value_in_list(tuple_list):\n",
    "    \\\"\\\"\\\"\n",
    "    This function finds the maximum value in a list of tuples.\n",
    "\n",
    "    Parameters:\n",
    "    tuple_list (list): A list of tuples.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The maximum value in the list of tuples.\n",
    "    \\\"\\\"\\\"\n",
    "    if not tuple_list:\n",
    "        return None\n",
    "    max_value = max(tuple_list, key=lambda x: x[1])\n",
    "    return max_value\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 12\n",
      "[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])] [('key1', 5), ('key2', 4), ('key3', 9)]\n"
     ]
    }
   ],
   "source": [
    "i = 12\n",
    "print(\"Problem\", i)\n",
    "sample_problem = mbpp['train'][i] \n",
    "problem_description = sample_problem[\"text\"]\n",
    "ground_truth_code = sample_problem[\"code\"]\n",
    "test_cases = sample_problem[\"test_list\"]\n",
    "input_data, expected_output = parse_test_case(test_cases[0])\n",
    "print(input_data, expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def maximum_value(test_list):\n",
      "  res = [(key, max(lst)) for key, lst in test_list]\n",
      "  return (res) \n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m local_env \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m exec(ground_truth_code, {}, local_env)\n\u001b[0;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_env\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m     12\u001b[0m exec(c, {}, local_env)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])]\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for test_case in test_cases:\n",
    "    input_data = '''[('key1', [3, 4, 5]), ('key2', [1, 4, 2]), ('key3', [9, 3])]'''\n",
    "    expected_output = \"[('key1', 5), ('key2', 4), ('key3', 9)]\"\n",
    "    #input_data, expected_output = parse_test_case(test_case)\n",
    "    # Use `exec` to run the code and check the output\n",
    "    local_env = {}\n",
    "\n",
    "    exec(ground_truth_code, {}, local_env)\n",
    "    result = local_env[input_data]\n",
    "    print(result)\n",
    "\n",
    "    exec(c, {}, local_env)\n",
    "    result = local_env[input_data]\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def maximum_value(test_list):\n",
      "  res = [(key, max(lst)) for key, lst in test_list]\n",
      "  return (res) \n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during function execution: not enough values to unpack (expected 2, got 1)\n"
     ]
    }
   ],
   "source": [
    "local_env = {}\n",
    "\n",
    "try:\n",
    "    # Dynamically execute the function string\n",
    "    exec(ground_truth_code, {}, local_env)\n",
    "\n",
    "    # Extract the function name (assumes it's the first 'def' in the string)\n",
    "    function_name = ground_truth_code.split(\"def \")[1].split(\"(\")[0].strip()\n",
    "\n",
    "    # Call the function dynamically with the input\n",
    "    result = local_env[function_name](input_data)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error during function execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(ground_truth_code, {}, local_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(ground_truth_code, {}, local_env)\n",
    "function_handle = next(iter(local_env.values()))\n",
    "result = function_handle(eval(input_data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key1', 5), ('key2', 4), ('key3', 9)]\n"
     ]
    }
   ],
   "source": [
    "result = function_handle(eval(input_data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function(predicted_code):\n",
    "    \"\"\"\n",
    "    Extracts a function from a code string starting with 'def' and ending based on indentation rules.\n",
    "    \"\"\"\n",
    "    lines = predicted_code.splitlines()  # Split code into lines\n",
    "    start_index = -1\n",
    "\n",
    "    # Find the first occurrence of a line starting with 'def'\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"def \"):  # Check for the 'def' keyword\n",
    "            start_index = i\n",
    "            break\n",
    "\n",
    "    if start_index == -1:\n",
    "        return \"No function definition found.\"\n",
    "\n",
    "    # Determine indentation of the function header\n",
    "    function_indent = len(lines[start_index]) - len(lines[start_index].lstrip())\n",
    "    function_lines = [lines[start_index]]  # Start collecting the function from 'def'\n",
    "\n",
    "    # Collect all indented lines after the function header\n",
    "    for line in lines[start_index + 1:]:\n",
    "        current_indent = len(line) - len(line.lstrip())\n",
    "\n",
    "        # Stop if indentation decreases or line is blank and isn't a continuation\n",
    "        if current_indent <= function_indent:\n",
    "            break\n",
    "\n",
    "        function_lines.append(line)\n",
    "\n",
    "    # Join the collected lines to reconstruct the function\n",
    "    return \"\\n\".join(function_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 5), ('key2', 4), ('key3', 9)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maximum_value(test_list):\n",
    "  res = [(key, max(lst)) for key, lst in test_list]\n",
    "  return (res) \n",
    "\n",
    "maximum_value([(\"key1\", [3, 4, 5]), (\"key2\", [1, 4, 2]), (\"key3\", [9, 3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a single Python function to solve the problem above.\n",
      "\n",
      "```python\n",
      "def find_max_value_in_list(tuple_list):\n",
      "    \"\"\"\n",
      "    This function finds the maximum value in a list of tuples.\n",
      "\n",
      "    Parameters:\n",
      "    tuple_list (list): A list of tuples.\n",
      "\n",
      "    Returns:\n",
      "    tuple: The maximum value in the list of tuples.\n",
      "    \"\"\"\n",
      "    if not tuple_list:\n",
      "        return None\n",
      "    max_value = max(tuple_list, key=lambda x: x[1])\n",
      "    return max_value\n",
      "\n",
      "# Example usage:\n",
      "tuple_list = [(1, 2), (3, 4), (5, 6), (7, 8)]\n",
      "max_value = find_max_value_in_list(tuple_list)\n",
      "print(max_value)  # Output: (7, 8)\n",
      "```\n",
      "\n",
      "This solution works by using the built-in `max` function in Python, which returns the largest item in an iterable. The `key` argument of the `max` function is used to specify that we want to compare the tuples based on their values (i.e., the second element of each tuple). The `max_value` variable is assigned the maximum tuple found in the list. Finally, the function returns `max_value`. The example usage demonstrates how to call the function with a list of tuples and print the result.\n"
     ]
    }
   ],
   "source": [
    "print(predicted_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
