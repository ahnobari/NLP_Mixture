{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from DataUtils import *\n",
    "from hfUtils import *\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Load The Data to RAM\n",
    "Below is the function to call to get the data loaded and formatted into prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts, test_prompts, subjects = load_mmlu_to_ram('./MMLU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: professional psychology\n",
      "--------------------\n",
      "Training Prompt:\n",
      "The following are multiple choice questions (with answers) about professional psychology.\n",
      "\n",
      "One of your therapy clients asks your advice about a good weight- reduction program. You have investigated the programs in the community and are enrolled in the one you consider the best. This program offers a $50 bonus to its patrons for each new person they bring into the program. Under these circumstances, your most appropriate response would be to\n",
      "A. tell your client the pros and cons of each program you know about except for the one in which you are enrolled\n",
      "B. recommend to your client the program in which you are enrolled and explain the $50 bonus you will receive\n",
      "C. recommend to your client the program in which you are enrolled and offer to have the $50 bonus credited to your client's account in the program\n",
      "D. tell your client the pros and cons of each program you know about, but do not claim the $50 bonus if your client enrolls in your program\n",
      "Answer: D\n",
      "\n",
      "There are three ways to measure the Central Tendency: the Mean, the Median and the Mode. From your knowledge about them, what is the mode?\n",
      "A. less sensitive to extreme scores than the mean\n",
      "B. more useful for skewed distributions\n",
      "C. sensitive to extreme values and highly skewed distributions\n",
      "D. the most frequently occurring number\n",
      "Answer: D\n",
      "\n",
      "In terms of Hofstede’s (1980) five cultural dimensions, the United States scores at the top of the scale on:\n",
      "A. individualism.\n",
      "B. individualism and power distance.\n",
      "C. power distance and masculinity.\n",
      "D. uncertainty avoidance.\n",
      "Answer: A\n",
      "\n",
      "Carl Jung believed that a client's transference:\n",
      "A. is a fantasy that distracts the client from reality.\n",
      "B. represents “mixed feelings” toward the therapist. \n",
      "C. \"is a form of \"\"acting out.\"\"\"\n",
      "D. reflects the client’s personal and collective unconscious.\n",
      "Answer: D\n",
      "\n",
      "In the construction of a multiple regression equation for purposes of prediction, the optimal combination of measures is one in which the predictors\n",
      "A. are uncorrelated with each other but are moderately correlated with the criterion\n",
      "B. have low correlations with each other and low correlations with the criterion\n",
      "C. are highly intercorrelated with each other and moderately correlated with the criterion\n",
      "D. have low correlations with the criterion bur are moderately correlated with each other\n",
      "Answer: A\n",
      "\n",
      "\n",
      "--------------------\n",
      "Test Question: You receive a call from Mrs. Wang who is very upset because her 14-year-old daughter witnessed the assault of her best friend three days ago and is very distraught. The girl hasn’t slept, won’t eat, and can’t stop crying. You have limited experience working with adolescents and in providing crisis intervention services. However, there is no one else in the community who is more experienced than you are. As an ethical psychologist, you will:\n",
      "A. agree to see Mrs. Wang’s daughter in therapy since you’ve had some experience providing crisis intervention experiences.\n",
      "B. inform Mrs. Wang about your lack of experience and let her decide if she wants you to provide therapy to her daughter.\n",
      "C. inform Mrs. Wang that you cannot see her daughter because of your lack of experience.\n",
      "D. see Mrs. Wang’s daughter in therapy only until the crisis has ended or until you locate alternative services.\n",
      "Answer: \n",
      "Expected Answer: D\n",
      "\n",
      "\n",
      "Subject: sociology\n",
      "--------------------\n",
      "Training Prompt:\n",
      "The following are multiple choice questions (with answers) about sociology.\n",
      "\n",
      "Which of the following did the post-war welfare state of 1948 not aim to provide:\n",
      "A. free health care and education for all\n",
      "B. a minimum wage\n",
      "C. full employment\n",
      "D. universal welfare\n",
      "Answer: B\n",
      "\n",
      "What does Berger (1963) describe as a metaphor for social reality?\n",
      "A. a fairground ride\n",
      "B. a circus\n",
      "C. a puppet theatre\n",
      "D. a ballet\n",
      "Answer: C\n",
      "\n",
      "The shift from 'civil religion' to 'common religion' means that:\n",
      "A. the increasing bureaucracy of the state has made religion only a marginal part of our lives\n",
      "B. despite the weakening of traditional authority, our everyday lives and 'common sense' remain shaped by religious beliefs and values\n",
      "C. religious participation in collective worship may have declined, but people still practise their faiths in private\n",
      "D. people are much more likely to discuss their religious beliefs in public, informal settings\n",
      "Answer: B\n",
      "\n",
      "The term 'hegemony' refers to:\n",
      "A. the tendency for the working class not to realize their own interests\n",
      "B. a dominant ideology that legitimates economic, political and cultural power\n",
      "C. a form of dual consciousness based on ideology and everyday experiences\n",
      "D. a mode of payment given for outstanding topiary\n",
      "Answer: B\n",
      "\n",
      "Which of the following is not a problem associated with official statistics on strike action?\n",
      "A. most strikes go unnoticed by employers and the mass media\n",
      "B. not all industrial disputes will be reported by the employer\n",
      "C. the definition of strikes excludes those that involve fewer than ten workers or last less than one day\n",
      "D. it is hard to compare strikes that were measured in different ways\n",
      "Answer: A\n",
      "\n",
      "\n",
      "--------------------\n",
      "Test Question: Pluralist theories suggest that:\n",
      "A. the state's power can be exercised through several different administrative structures\n",
      "B. the ruling elite is composed of people from various class backgrounds\n",
      "C. political parties must compete for the votes of 'consumers' in the electorate\n",
      "D. there is a close alignment between class background and party preference\n",
      "Answer: \n",
      "Expected Answer: C\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_examples = 2\n",
    "n_smaples = 1\n",
    "\n",
    "for i in range(n_examples):\n",
    "    rnd_subject = np.random.choice(len(subjects))\n",
    "    \n",
    "    print('Subject:', subjects[rnd_subject])\n",
    "    print(\"--------------------\")\n",
    "    print(\"Training Prompt:\")\n",
    "    print(train_prompts[rnd_subject])\n",
    "    print('--------------------')\n",
    "    \n",
    "    for j in range(n_smaples):\n",
    "        rnd_question = np.random.choice(len(test_prompts[rnd_subject]))\n",
    "        \n",
    "        print('Test Question:', test_prompts[rnd_subject][rnd_question][0])\n",
    "        print('Expected Answer:', test_prompts[rnd_subject][rnd_question][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing A Model\n",
    "For now let's test google/gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 17:42:49.683750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-10 17:42:49.689879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731278569.696750 3314381 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731278569.698786 3314381 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-10 17:42:49.706251: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8ed16f47854e58889e2a460069a4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_model_and_tokenizer(\"google/gemma-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa2231c7d5a4f519c51c43838e61251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject abstract algebra accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153313ea348243e6ad2e356ea6417c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject anatomy accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d3f2ff78274d1ca113d582a7fd8785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject astronomy accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a31f24503ee4b8f91fae2705651288f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids)\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m probs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     17\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m     26\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m pred \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m}[np\u001b[38;5;241m.\u001b[39margmax(probs)]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# output = model.generate(input_ids, max_length=input_ids.shape[1]+1, num_return_sequences=1, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# prediction = tokenizer.decode(output[0], skip_special_tokens=True)[len(input_prompt):]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run all test prompts through the model and get the predictions\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    accuracies = []\n",
    "    for i,subject in enumerate(subjects):\n",
    "        train_prompt = train_prompts[i]\n",
    "        predictions.append([])\n",
    "        accuracies.append(0)\n",
    "        for j in tqdm(range(len(test_prompts[i]))):\n",
    "            input_prompt = train_prompt + test_prompts[i][j][0]\n",
    "            expected_output = test_prompts[i][j][1]\n",
    "            \n",
    "            input_ids = tokenizer.encode(input_prompt, return_tensors='pt').to(model.device)\n",
    "            logits = model(input_ids=input_ids).logits[0, -1]\n",
    "            \n",
    "            probs = (\n",
    "                torch.nn.functional.softmax(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            logits[tokenizer(\"A\").input_ids[-1]],\n",
    "                            logits[tokenizer(\"B\").input_ids[-1]],\n",
    "                            logits[tokenizer(\"C\").input_ids[-1]],\n",
    "                            logits[tokenizer(\"D\").input_ids[-1]],\n",
    "                        ]\n",
    "                    ).float(),\n",
    "                    dim=0,\n",
    "                )\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            pred = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[np.argmax(probs)]\n",
    "        \n",
    "            # output = model.generate(input_ids, max_length=input_ids.shape[1]+1, num_return_sequences=1, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "            # prediction = tokenizer.decode(output[0], skip_special_tokens=True)[len(input_prompt):]\n",
    "            \n",
    "            predictions[-1].append(pred)\n",
    "            \n",
    "            if pred == expected_output:\n",
    "                accuracies[-1] += 1\n",
    "                \n",
    "        accuracies[-1] /= len(test_prompts[i])\n",
    "        print(f\"Subject {subject} accuracy: {accuracies[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT stands for Massachusetts Institute of Technology. It is a private research university in Cambridge, Massachusetts, United States. MIT is one of the world's leading research universities, with a wide range of programs in science, technology, engineering, and medicine.\n"
     ]
    }
   ],
   "source": [
    "# print out sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 2\n",
    "n_smaples = 1\n",
    "\n",
    "for i in range(n_examples):\n",
    "    rnd_subject = np.random.choice(len(subjects))\n",
    "    \n",
    "    print('Subject:', subjects[rnd_subject])\n",
    "    print(\"--------------------\")\n",
    "    print(\"Training Prompt:\")\n",
    "    print(train_prompts[rnd_subject])\n",
    "    print('--------------------')\n",
    "    \n",
    "    for j in range(n_smaples):\n",
    "        rnd_question = np.random.choice(len(test_prompts[rnd_subject]))\n",
    "        \n",
    "        print('Test Question:', test_prompts[rnd_subject][rnd_question][0])\n",
    "        print('Expected Answer:', test_prompts[rnd_subject][rnd_question][1])\n",
    "        print('Predicted Answer:', predictions[rnd_subject][rnd_question])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
