# Activation Relaxed LLM Merging
This repository is the code base for the Activation Aware LLM Merging.

Model merging, a technique that combines parameters and embeddings from multiple models, has emerged as a promising strategy to enhance large language model (LLM) performance across diverse tasks while maintaining computational efficiency. Despite the increasing availability of fine-tuned, task-specific checkpoints from public repositories, their potential for task-agnostic merging remains underexplored. In this study, we investigate the feasibility and performance of creating a unified "soup of LLMs" by combining publicly available checkpoints through various merging methods, including Simple Weight Averaging, Arithmetic Mixture, DARE, TIES-Merging, sparse mask merging, and WIDEN. We evaluate the merged models on benchmarks including MMLU, MBPP, and Math, providing a diverse assessment of their performance across tasks. However, our findings reveal the fragility of merging methods when incorporating low-quality checkpoints. To address this, we introduce an activation-aware merging method that leverages a calibration set to prioritize critical weights during the merging process. By providing a comprehensive analysis of naive and filtered crowd-sourcing scenarios, our work highlights the limitations of existing methods and paves the way for developing more robust and effective model merging techniques.
